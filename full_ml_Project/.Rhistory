warnings()
modelFit <- train(classe ~. , data= yytrain, preProcess = c("center","scale"), method = "rpart");
modefit
modeFit
f<- train(classe~., data=yytrain, method = 'rf', trControl = trainControl(method='cv'))
warnings()
rm(list = ls());
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
# create training and test sets according the the Case variable.
set.seed(125)
#inTrain <- createDataPartition(segmentationOriginal$Case=="Train",  list =FALSE);
training<-subset(segmentationOriginal,segmentationOriginal$Case=="Train");
testing <- subset(segmentationOriginal,segmentationOriginal$Case=="Test");
# Fit a CART (Classification And Regression Model) to the training set
# using the train() in caret and (rpart method in the rpart package.)
str(training$Class)
data( iris )
d <- dummy.data.frame( iris )
get.dummy( d, 'Species' )
install.packages("dummies")
library(dummies);
data( iris )
d <- dummy.data.frame( iris )
get.dummy( d, 'Species' )
str(d)
letters <- c( "a", "a", "b", "c", "d", "e", "f", "g", "h", "b", "b" )
dummy( as.character(letters) )
dummy( letters[1:6] )
dummy =dummies( yytrain, name= yytrain$classe)
dummies =dummy( yytrain, name= yytrain$classe)
dummies =dummy(yytrain$classe)
rm(list=ls());
library(caret)
library(ggplot2)
library(forecast);
library(e1071)
library(RnavGraph)
library(Hmisc)
library(dummies);
# load data saved memory.
load("traindata");
load("testdata");
#Exploratory analysis
#str(traindata)
testing<-testdata;     # Reassign
training <- traindata;
## remove columns with at least NAs in both training and testing sets.
#testdf <-testing[ , colSums(is.na(test)) == 0];            # remove all cols from the testing se with NAs.
#traindf <- traindf[ , colSums(is.na(traindf)) == " "];
traindf <- training[ , colSums(is.na(training)) == 0];   # remove all columns with NAs from the training set
indexColsToRemove <- c(1,2,5:6,12:20,43:48,52:60,74:82)
colNamesToRemove <-names(traindf[,indexColsToRemove])
ColsToKeep <-setdiff(names(traindf),colNamesToRemove);
trainf <-traindf[,-indexColsToRemove];
names(trainf);
dim(training)
dim(traindf)
dim(trainf);
y = c(1,3,9,31,32,33,56);
## variables with percentUnique (9-15%);         y1=c(6,7,18,19,20,44,45,46,55)
yytrain <- trainf[,y]
# convert classe to dummies.
dummies =dummy(yytrain$classe)
str(dummies)
class(dummies)
yy<- data.frame(yytrain[,1:6],dummies)
str(yy)
x=as.factor(yytrain$classe)
str(x)
modelFit <- train(as.factor(lasse) ~. , data= yytrain, preProcess = c("center","scale"), method = "rpart");
modelFit <- train(as.factor(classe) ~. , data= yytrain, preProcess = c("center","scale"), method = "rpart");
str(modelFit)
summary(modelFit)
confusionMatrix(modelFit, yytrain$classe)
confusionMatrix(modelFit$finalModel, yytrain$classe)
predictions<-(modelFit, newdata=yytrain)
predictions<-predict(modelFit, newdata=yytrain)
summary(predictions)
confusionsMatrix(predictions,yytrain$classe)
confusionMatrix(predictions,yytrain$classe)
rm(list=ls());
library(caret)
library(ggplot2)
library(forecast);
library(e1071)
library(RnavGraph)
library(Hmisc)
library(dummies);
set.seed(7777)
# load data saved memory.
load("traindata");
load("testdata");
testing<-testdata;     # Reassign
training <- traindata;
traindf <- training[ , colSums(is.na(training)) == 0];   # remove all columns with NAs from the training set
indexColsToRemove <- c(1,2,5:6,12:20,43:48,52:60,74:82)
colNamesToRemove <-names(traindf[,indexColsToRemove])
ColsToKeep <-setdiff(names(traindf),colNamesToRemove);
trainf <-traindf[,-indexColsToRemove];
names(trainf);
dim(training)
dim(traindf)
dim(trainf);
plot(trainf[,32],type="l",col="blue")
y = c(1,3,9,31,32,33,56);
## variables with percentUnique (9-15%);         y1=c(6,7,18,19,20,44,45,46,55)
yytrain <- trainf[,y]
# convert classe to dummies.
dummies =dummy(yytrain$classe)
modelFit <- train(as.factor(classe) ~. , data= yytrain, preProcess = c("center","scale"), method = "rpart");
predictions<-predict(modelFit, newdata=yytrain)
confusionMatrix(predictions,yytrain$classe)
modelFit <- train(as.factor(classe) ~. , data= yytrain, preProcess = c("center","scale"), method = "rf");
finalTraining <-data.frame(classe=as.factor(yytrain$clasee),yytrain[,-7]);
classe <- as.factor(yytrain$classe);
finalTraining <-data.frame(classe,yytrain[,1:6]);
str(finalTraining)
cor(finalTraining[,2],finalTraining[,2])
cor(finalTraining[,2],finalTraining[,3])
modelFit <- train(as.factor(classe) ~. , data= finalTraining, preProcess = c("center","scale"), method = "glm");
modelFit <- train(classe ~. , data= finalTraining, preProcess = c("center","scale"), method = "glm");
str(finalTraining)
mod <- train(classe ~., data= finalTraining, method = "glm")
mod <- train(classe ~., data= finalTraining, method = "rpart")
predictions<-predict(mod, newdata=finalTraining)
confusionMatrix(predictions,finalTraining$classe)
f<- train(classe~., data=finalTraining, method = 'rf', trControl = trainControl(method='cv'))
str(f)
summary(f)
pred<-predict(f, newdata=finalTraining)
confusionMatrix(pred,finalTraining$classe)
5580+3797+3422+3216+3607
predOutOfSample <- predict(f,newdata = testing)
conf <- confusionMatrix(predOutOfSample,testing$classe)
conf <- confusionMatrix(predOutOfSample,as.factors(testing$classe));
conf <- confusionMatrix(predOutOfSample,as.factor(testing$classe));
str(testing)
names(testing)
names(finalTraining)
ttt <- subset(testing,select=c("raw_timestamp_part_1","num_window","gyros_belt_y","pitch_dumbbell", "yaw_dumbbell","total_accel_dumbbell"));
names(ttt)
str(ttt)
newtesting <- subset(testing,select=c("raw_timestamp_part_1","num_window","gyros_belt_y","pitch_dumbbell", "yaw_dumbbell","total_accel_dumbbell"));
predOutOfSample <- predict(f,newdata = newtesting)
conf <- confusionMatrix(predOutOfSample,newdta = as.factor(testing$classe));
str(preOutOfSample)
predOutOfSample <- predict(f,newdata = newtesting)
predOutOfSample
mypred<- predict(f,newdata = newtesting)
mypred
conf <-confusionMatrix(insamplePredictions,finalTraining$classe)
insamplePredtictions <-predict(finalMod, newdata=finalTraining)
predOutOfSample
sapply(finalTraining[,1:5],mean)
sapply(finalTraining[,2:7],mean)
traindf <- training[ , colSums(is.na(training)) == 0];   # remove all columns with NAs from the training set
novar <- nearZeroVar(trainf,saveMetrics=TRUE);
summary(novar)
table(novar)
head(table(novar))
head(table(novar),90)
novar
names(finalTraining)
names(yytrain)
effective <- c("raw_timestamp_part_2","roll_dumbbell ","pitch_dumbbell ","yaw_dumbbell")
effective <- c("classe","raw_timestamp_part_2","roll_dumbbell ","pitch_dumbbell ","yaw_dumbbell")
mytraining <- subset(training, select=effective)
names(training)
effective <- c("classe","raw_timestamp_part_2","roll_dumbbell","pitch_dumbbell","yaw_dumbbell")
mytraining <- subset(training, select=effective)
str(mytraining)
classe <- as.factor(training$classe)
effective <- c(raw_timestamp_part_2","roll_dumbbell","pitch_dumbbell","yaw_dumbbell")
effective <- c("raw_timestamp_part_2","roll_dumbbell","pitch_dumbbell","yaw_dumbbell")
mytraining <- data.frame(classe,subset(training, select=effective);
mytraining <- data.frame(classe,subset(training, select=effective));
str(mytraining)
effectiveData <- c("raw_timestamp_part_2","roll_dumbbell","pitch_dumbbell","yaw_dumbbell");
myTraining <- data.frame(classe,subset(training, select=effectiveData));
effectiveData <- c("raw_timestamp_part_2","roll_dumbbell","pitch_dumbbell","yaw_dumbbell");
classe <- as.factor(training$classe);
myTraining <- data.frame(classe,subset(training, select=effectiveData));
effectiveCols <- c("raw_timestamp_part_2","roll_dumbbell","pitch_dumbbell","yaw_dumbbell");
classe <- as.factor(training$classe);
myTraining <- data.frame(classe,subset(training, select=effectiveCols));
myTesting <- subset(testing, select = effective);
effectiveCols <- c("raw_timestamp_part_2","roll_dumbbell","pitch_dumbbell","yaw_dumbbell");
classe <- as.factor(training$classe);
myTraining <- data.frame(classe,subset(training, select=effectiveCols));
myTesting <- subset(testing, select = effective);
mod2<- train(classe~., data=myTraining, method = 'rf', trControl = trainControl(method='cv'));
pred2 <-predict(mod2, newdata=myTraining);
confMat2 <-confusionMatrix(pred2,myTraining$classe);
mypred2 <- predict(mod2, newdata= myTesting);
confMat2
confMatrix <-confusionMatrix(insamplePredictions,finalTraining$classe)
finalTraining <-data.frame(classe,yytrain[,1:6]);
insamplePredtictions <-predict(finalMod, newdata=finalTraining)
mypred2 <- predict(mod2, newdata= myTesting);
mypred2
finalMod<- train(classe~., data=finalTraining, method = 'rf', trControl = trainControl(method='cv'))
insamplePredtictions <-predict(finalMod, newdata=finalTraining)
confMatrix <-confusionMatrix(insamplePredictions,finalTraining$classe)
insamplePredtictions <-predict(finalMod, newdata=finalTraining)
confMatrix <-confusionMatrix( insamplePredtictions,finalTraining$classe)
confMatrix
myPredictions <- predict(finalMod,newdata = newTesting)
newTesting <- subset(testing,select=c("raw_timestamp_part_1","num_window","gyros_belt_y","pitch_dumbbell", "yaw_dumbbell","total_accel_dumbbell"));
myPredictions <- predict(finalMod,newdata = newTesting)
mypredictions
myPredictions
you don't want me to teach this kids and that's fine . it's just ok to let them know the real thing in the department of history and the necessary thing that we need to make come through and have to make it work in that regard.'
cls
l
mypred2 <- predict(mod2, newdata= myTesting);
mypred2
confMat2
myPredictions <- predict(finalMod,newdata = newTesting)
myPredictions
mypred2
qplot(finalTraining[,2],col=classe, data=finalTraining, geom="density") # nice plot
hist(finalTraining[,2])  # may need adjustment
hist(finalTraining[,3])  # kind of okay
hist(finalTraining[,4])  # okay
hist(finalTraining[,5])  # may need transformation
hist(finalTraining[,6])  # may need transformation.
qplot(finalTraining[,2],col=classe, data=finalTraining, geom="density") # nice plot
qplot(finalTraining[,3],col=classe, data=finalTraining, geom="density")
mlulr <- http://archive.ics.uci.edu/ml
mlulr <- "http://archive.ics.uci.edu/ml/";
novar <- nearZeroVar(traindf,saveMetrics=TRUE);
set.seed(3523)
library(AppliedPredictiveModeling)
require(e1071)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
svm.model <- svm(CompressiveStrength ~ ., data = training)
svm.pred <- predict(svm.model, testing[,-9])
accuracy(svm.pred,testing$CompressiveStrength)
library(forecast)
library(caret)
library(AppliedPredictiveModeling)
require(e1071)
library(forecast)
library(caret)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
svm.model <- svm(CompressiveStrength ~ ., data = training)
svm.pred <- predict(svm.model, testing[,-9])
accuracy(svm.pred,testing$CompressiveStrength)
answers <- c("B", "A", "B", "A", "A", "E", "D", "B", "A", "A", "B", "C", "B", "A", "E", "E", "A", "B", "B", "B")
answers
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
pml_write_files(answers)
class(answers)
print(modelFit$finalModel);
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
# create training and test sets according the the Case variable.
set.seed(125)
#inTrain <- createDataPartition(segmentationOriginal$Case=="Train",  list =FALSE);
training<-subset(segmentationOriginal,segmentationOriginal$Case=="Train");
testing <- subset(segmentationOriginal,segmentationOriginal$Case=="Test");
# Fit a CART (Classification And Regression Model) to the training set
# using the train() in caret and (rpart method in the rpart package.)
modelFit <- train(Class ~., method ="rpart", data = training)
predictions <-predict(modelFit,newdata=testing)
# look at the fitted model, tells what all the nodes are ..etc
print(modelFit$finalModel);
require(rattle)
fancyRpartPlot(modelFit$finalModel);
install.packages("xtable")
novar <- nearZeroVar(traindf,saveMetrics=TRUE);
library(caret)
library(ggplot2)
library(forecast);
library(e1071)
library(RnavGraph)
library(Hmisc)
library(dummies);
set.seed(7777)
# load data saved memory.
load("traindata");
load("testdata");
#Exploratory analysis
#str(traindata)
testing<-testdata;     # Reassign
training <- traindata;
## remove columns with at least NAs in both training and testing sets.
#testdf <-testing[ , colSums(is.na(test)) == 0];            # remove all cols from the testing se with NAs.
#traindf <- traindf[ , colSums(is.na(traindf)) == " "];
traindf <- training[ , colSums(is.na(training)) == 0];   # remove all columns with NAs from the training set
novar <- nearZeroVar(traindf,saveMetrics=TRUE);
novar
library(knitr)
library(caret)
library(ggplot2)
library(forecast)
library(Hmisc)
library(xtable)
library(e1071)
library(knitr)
#trainUrl<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv";
#traindata <- read.csv(trainUrl,as.is=TRUE);
## get test data
#testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv";
#testdata <- read.csv(testUrl, as.is = TRUE)
#save(traindata, file="traindata"); # save data on local drive.
#save(testdata, file="testdata");
traindf <- training[ , colSums(is.na(training)) == 0]; # remove all columns with NAs from the training set
novar <- nearZeroVar(traindf,saveMetrics=TRUE);
indexColsToRemove <- c(1,2,5:6,12:20,43:48,52:60,74:82)
colNamesToRemove <-names(traindf[,indexColsToRemove])
ColsToKeep <-setdiff(names(traindf),colNamesToRemove);
trainf <-traindf[,-indexColsToRemove];
names(trainf);
dim(training)
dim(traindf)
dim(trainf);
tempTraining <- training[ , colSums(is.na(training)) == 0]; # remove all columns with NAs from the training set
novar <- nearZeroVar(tempTraining,saveMetrics=TRUE);
indexColsToRemove <- c(1,2,5:6,12:20,43:48,52:60,74:82)
colNamesToRemove <-names(tempTraining[,indexColsToRemove])
ColsToKeep <-setdiff(names(tempTraining),colNamesToRemove);
trainf <-tempTraining[,-indexColsToRemove];
names(trainf);
dim(training)
dim(tempTraining)
dim(trainf);
library(caret)
library(ggplot2)
library(forecast)
library(Hmisc)
library(xtable)
library(e1071)
library(knitr)
#trainUrl<- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv";
#traindata <- read.csv(trainUrl,as.is=TRUE);
## get test data
#testUrl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv";
#testdata <- read.csv(testUrl, as.is = TRUE)
#save(traindata, file="traindata"); # save data on local drive.
#save(testdata, file="testdata");
tempTraining <- training[ , colSums(is.na(training)) == 0]; # remove all columns with NAs from the training set
novar <- nearZeroVar(tempTraining,saveMetrics=TRUE);        # checking variables which have zero variances.
indexColsToRemove <- c(1,2,5:6,12:20,43:48,52:60,74:82)     # columns to further remove from the dataframe.
colNamesToRemove <-names(tempTraining[,indexColsToRemove])
ColsToKeep <-setdiff(names(tempTraining),colNamesToRemove);
trainf <-tempTraining[,-indexColsToRemove];
names(trainf);
dim(training)
dim(tempTraining)
dim(trainf);
library(Hmisc)
highVarianceIndex = c(1,3,9,31,32,33,56);   ## variables with high percentUnique(above 70%).
tempTraining <- training[ , colSums(is.na(training)) == 0]; # remove all columns with NAs from the training set
novar <- nearZeroVar(tempTraining,saveMetrics=TRUE);        # checking variables which have zero variances.
indexColsToRemove <- c(1,2,5:6,12:20,43:48,52:60,74:82)     # columns to further remove from the dataframe.
tempTraining <-tempTraining[,-indexColsToRemove];
dim(training)
dim(tempTraining)
corr<- cor(tempTraining[,1:55]);  # calculate the correction of the variables
# check variables with more unique values
novar <- nearZeroVar(tempTraining,saveMetrics=TRUE);
highVarianceIndex = c(1,3,9,31,32,33,56);   ## variables with high percentUnique(above 70%).
tempTraining <- tempTraining[,y];
tempTraining <- tempTraining[, highVarianceIndex];
classe <- as.factor(yytrain$classe);
classe <- as.factor(tempTraining$classe);
dim(tempTraining)
str(tempTraining)
finalTraining <-data.frame(classe,tempTraining[,-7]);
str(finalTraining)
inTrain <-createDataPartition(y=finalTraining$classe, p=.75,list = FALSE);
cvTraining <- finalTraining[inTrain, ];
cvTesting <- finalTrainig[-inTrain, ];
cvTesting <- finalTraining[-inTrain, ];
dim(cvTraining);
dim(cvTesting);
predictInSample <- predict(modelFit, newdata = cvTraining);  #in- subsample prediction
predictOutSample <- predict(modelFit, newdata = cvTesting);
confInSample <- confusionMatrix(predictInSample, cvTraining$classe);
names(finalTraining)
str(confOutSample)
finalTesting <- subset(testing,select=c("raw_timestamp_part_1","num_window","gyros_belt_y","pitch_dumbbell", "yaw_dumbbell","total_accel_dumbbell"));
myPredictions <- predict(modelFit,newdata = newTesting)
myPredictions <- predict(modelFit,newdata = finalTesting)
n
myPredictions <- predict(modelFit,newdata = finalTesting)
finalTesting <- subset(testing,select=c("raw_timestamp_part_1","num_window","gyros_belt_y","pitch_dumbbell", "yaw_dumbbell","total_accel_dumbbell"));
myPredictions <- predict(modelFit,newdata = finalTesting)
str(finalTesting)
finalTesting <- subset(testing,select=c("raw_timestamp_part_1","num_window","gyros_belt_y","pitch_dumbbell", "yaw_dumbbell","total_accel_dumbbell"));
myPredictions <- predict(modelFit,newdata = finalTesting)
finalTesting <- subset(testing,select=c("raw_timestamp_part_1","num_window","gyros_belt_y","pitch_dumbbell", "yaw_dumbbell","total_accel_dumbbell"));
myPredictions <- predict(modelFit,newdata = finalTesting)
finalPredictions
getwd()
library(caret)
library(ggplot2)
library(forecast);
library(e1071)
library(RnavGraph)
library(Hmisc)
library(dummies);
set.seed(7777)
# load data saved memory.
load("traindata");
load("testdata");
#Exploratory analysis
#str(traindata)
testing<-testdata;     # Reassign
training <- traindata;
## remove columns with at least NAs in both training and testing sets.
#testdf <-testing[ , colSums(is.na(test)) == 0];            # remove all cols from the testing se with NAs.
#traindf <- traindf[ , colSums(is.na(traindf)) == " "];
traindf <- training[ , colSums(is.na(training)) == 0];   # remove all columns with NAs from the training set
novar <- nearZeroVar(traindf,saveMetrics=TRUE);
indexColsToRemove <- c(1,2,5:6,12:20,43:48,52:60,74:82)
colNamesToRemove <-names(traindf[,indexColsToRemove])
ColsToKeep <-setdiff(names(traindf),colNamesToRemove);
trainf <-traindf[,-indexColsToRemove];
names(trainf);
dim(training)
dim(traindf)
dim(trainf);
corr<- cor(trainf[,1:55])  # calculate the correction of the variables
# check variables with more unique values
novar <- nearZeroVar(trainf,saveMetrics=TRUE);
## variables with high percentUnique(above 70%).  y = c(1,3,9,31,32,33,57); 56 is classe
y = c(1,3,9,31,32,33,56);
## variables with percentUnique (9-15%);         y1=c(6,7,18,19,20,44,45,46,55)
yytrain <- trainf[,y];
classe <- as.factor(yytrain$classe);
finalTraining <-data.frame(classe,yytrain[,1:6]);
qplot(yaw_dumbbell,classe,data = finalTraining,colour=classe);
hist(finalTraining[,2])  # may need adjustment
hist(finalTraining[,3])  # kind of okay
hist(finalTraining[,4])  # okay
hist(finalTraining[,5])  # may need transformation
hist(finalTraining[,6])  # may need transformation.
qplot(finalTraining[,2],col=classe, data=finalTraining, geom="density") # nice plot
qplot(finalTraining[,3],col=classe, data=finalTraining, geom="density")
qplot(finalTraining[,2:6],col=classe, data=finalTraining, geom="density") # nice plot
qplot(finalTraining[,2],col=classe, data=finalTraining, geom="density") # nice plot
finalMod<- train(classe~., data=finalTraining, method = 'rf', trControl = trainControl(method='cv'))
oldpar <-par()
par(mfrow = c(3,3))
qplot(finalTraining[,2],col=classe, data=finalTraining, geom="density") # nice plot
qplot(finalTraining[,3],col=classe, data=finalTraining, geom="density")
qplot(finalTraining[,4],col=classe, data=finalTraining, geom="density")
qplot(finalTraining[,5],col=classe, data=finalTraining, geom="density")
qplot(finalTraining[,6],col=classe, data=finalTraining, geom="density")
par(oldpar)
oldpar <-par()
par(mfrow = c(3,3))
qplot(finalTraining[,2],col=classe, data=finalTraining, geom="density") # nice plot
qplot(finalTraining[,3],col=classe, data=finalTraining, geom="density")
qplot(finalTraining[,4],col=classe, data=finalTraining, geom="density")
qplot(finalTraining[,5],col=classe, data=finalTraining, geom="density")
qplot(finalTraining[,6],col=classe, data=finalTraining, geom="density")
oldpar <- par()
par(mfrow = c(3,3))
qplot(finalTraining[,2],col=classe, data=finalTraining, geom="density") # nice plot
qplot(finalTraining[,3],col=classe, data=finalTraining, geom="density")
qplot(finalTraining[,4],col=classe, data=finalTraining, geom="density")
qplot(finalTraining[,5],col=classe, data=finalTraining, geom="density")
qplot(finalTraining[,6],col=classe, data=finalTraining, geom="density")
par(oldpar)
dim(training)
testing<-testdata;     # Reassign
training <- traindata;
## remove columns with at least NAs in both training and testing sets.
#testdf <-testing[ , colSums(is.na(test)) == 0];            # remove all cols from the testing se with NAs.
#traindf <- traindf[ , colSums(is.na(traindf)) == " "];
traindf <- training[ , colSums(is.na(training)) == 0];   # remove all columns with NAs from the training set
str(trainf)
str(trindf)
str(traindf)
str(finalTRaining)
str(finalTraining)
plot(conMatrix$table)
plot(confMatrix$table)
finalMod<- train(classe~., data=finalTraining, method = 'rf', trControl = trainControl(method='cv'))
insamplePredtictions <-predict(finalMod, newdata=finalTraining)
confMatrix <-confusionMatrix(insamplePredtictions,finalTraining$classe)
#this is the real prediction for submission.
newTesting <- subset(testing,select=c("raw_timestamp_part_1","num_window","gyros_belt_y","pitch_dumbbell", "yaw_dumbbell","total_accel_dumbbell"));
confMatrix
plot(confMatrix$byClass)
pairs(confMatrix$table)
newTesting <- subset(testing,select=c("raw_timestamp_part_1","num_window","gyros_belt_y","pitch_dumbbell", "yaw_dumbbell","total_accel_dumbbell"));
myPredictions <- predict(finalMod,newdata = newTesting)
table(myPredictions)
plot(myPredictions)
plot(finalTraining)
hist(finalTraining)
setwd("~/ml_Project")
